{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typo-Corrector\n",
    "\n",
    "After I bought new MacBook, I lost my dominate over the new keyboard and I have a lot of typo during typing. What is interesting in this scenario is that always the mis-characters are in the neighborhood. So I thought a simple Autoencoder should learn this behavior easily.\n",
    "\n",
    "Then I decided to implement this Jupiter file to see the result. Also, it could be a good exercise :).\n",
    "So let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to have all possible character which can by typed mistakenly by neighbor character. Here I defined a dictionary which the key is the character, and values are possible mistake characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dict ={'q':\"wsa\",\n",
    "            'w':\"qase\",\n",
    "            'e':\"wsdr\",\n",
    "            'r':\"edft\",\n",
    "            't':\"rfgy\",\n",
    "            'y':\"tghu\",\n",
    "            'u':\"yhji\",\n",
    "            'i':\"ujko\",\n",
    "            'o':\"iklp\",\n",
    "            'p':\"ol\",\n",
    "            'a':\"qsz\",\n",
    "            's':\"adeqwxz\",\n",
    "            'd':\"esxcfr\",\n",
    "            'f':\"rdcvgt\",\n",
    "            'g':\"tfvbhy\",\n",
    "            'h':\"ygbnju\",\n",
    "            'j':\"uhnmki\",\n",
    "            'k':\"jmloi\",\n",
    "            'l':\"kmp\",\n",
    "            'z':\"asx\",\n",
    "            'x':\"zsdc\",\n",
    "            'c':\"xdfv\",\n",
    "            'v':\"cfgb\",\n",
    "            'b':\"vghn\",\n",
    "            'n':\"bhjm\",\n",
    "            'm':\"njkl\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to have some words to examine out the idea, the best source is a pre-trained word embedding. I used GloVe. Note that we only need words, not the weights. So I used 50d GloVe word to vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "with open('glove.6B.50d.txt', mode='r') as file:\n",
    "    for line in file:\n",
    "        values = line.strip().split(' ')\n",
    "        word = values[0].lower()\n",
    "        if len(word) < 3:\n",
    "            continue\n",
    "        vocab.append(word)\n",
    "        if len(vocab) == 10000:\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to produce the raw data. To do that, I defined a function which gets a word as an input and then loops over its characters and replaces this character by mistaken characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mismaker(word):\n",
    "    mis_words = []\n",
    "    for i, w in enumerate(list(word)):\n",
    "        index = char_dict.get(w)\n",
    "        if index is not None:\n",
    "            mischars = char_dict[w]\n",
    "\n",
    "            for m in mischars:\n",
    "                mis_word = word[:i] + m + word[i + 1:]\n",
    "                mis_words.append(mis_word)\n",
    "    \n",
    "    return mis_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for word in vocab:\n",
    "    row = [word]\n",
    "    mis_words = mismaker(word)\n",
    "    row.extend(mis_words)\n",
    "    data.append(row)\n",
    "\n",
    "# save data on the disk\n",
    "with open('data.txt', mode='w') as file:\n",
    "    for row in data:\n",
    "        line = ' '.join(row)\n",
    "        file.write(line+'\\n')\n",
    "del(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two empty lists for the inputs and the outputs. The inputs are typo words, and outputs are correct words. In autoencoders, we need to provide start and end of sequences. Here, Sequences are a series of characters. The encoder part needs to know where the sequences are ended and decoder needs the start of sequences and end of sequences. I demonstrate it in below picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/typo.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  286591 typo words\n"
     ]
    }
   ],
   "source": [
    "inputs_data = []\n",
    "outputs_data = []\n",
    "with open('data.txt', mode='r') as file:\n",
    "    for line in file:\n",
    "        words = line.strip().split(' ')\n",
    "        correct_word = words[0]\n",
    "        for i in range(1,len(words)):\n",
    "            inputs_data.append(words[i])\n",
    "            outputs_data.append('\\t'+correct_word+'\\n')\n",
    "            \n",
    "\n",
    "assert len(inputs_data)==len(outputs_data)\n",
    "print(\"There are \", len(inputs_data), \"typo words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "from random import shuffle\n",
    "zipped = list(zip(inputs_data,outputs_data))\n",
    "shuffle(zipped)\n",
    "inputs_data, outputs_data = zip(*zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', '$', '&', \"'\", '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ';', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'é', '’']\n"
     ]
    }
   ],
   "source": [
    "# extract unique characters\n",
    "chars = set()\n",
    "for word in outputs_data:\n",
    "    for c in word:\n",
    "        if c not in chars:\n",
    "            chars.add(c)\n",
    "\n",
    "chars = sorted(chars)\n",
    "\n",
    "# create a character to index dictionary\n",
    "char2idx = dict()\n",
    "for c in chars:\n",
    "    char2idx[c] = len(char2idx)\n",
    "\n",
    "# create a index to character dictionary\n",
    "idx2char = {v:k for k,v in char2idx.items()}\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras seq2seq\n",
    "To make life easier, I used [keras sequential model](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html). If you don't have info, I highly recommend you to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens = num_decoder_tokens = len(char2idx)\n",
    "max_encoder_seq_length = max([len(word) for word in inputs_data])\n",
    "max_decoder_seq_length = max([len(word) for word in outputs_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(inputs_data), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(inputs_data), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(inputs_data), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_word, output_word) in enumerate(zip(inputs_data, outputs_data)):\n",
    "    for t, char in enumerate(input_word):\n",
    "        encoder_input_data[i, t, char2idx[char]] = 1.\n",
    "    for t, char in enumerate(output_word):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, char2idx[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, char2idx[char]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "I used only first 5000 words from GloVe word embedding. If we have a larger data, e.g 100K words, we should use a larger LSTM unit. I choose this setting, because of training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "latent_dim = 128\n",
    "batch_size = 512  \n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 229272 samples, validate on 57319 samples\n",
      "Epoch 1/50\n",
      "229272/229272 [==============================] - 20s 88us/step - loss: 1.0376 - val_loss: 0.8413\n",
      "Epoch 2/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.6844 - val_loss: 0.5513\n",
      "Epoch 3/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.4756 - val_loss: 0.4041\n",
      "Epoch 4/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.3529 - val_loss: 0.3251\n",
      "Epoch 5/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.2770 - val_loss: 0.2367\n",
      "Epoch 6/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.2255 - val_loss: 0.1945\n",
      "Epoch 7/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.1898 - val_loss: 0.1663\n",
      "Epoch 8/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.1636 - val_loss: 0.1436\n",
      "Epoch 9/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.1435 - val_loss: 0.1654\n",
      "Epoch 10/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.1279 - val_loss: 0.1179\n",
      "Epoch 11/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.1155 - val_loss: 0.1093\n",
      "Epoch 12/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.1049 - val_loss: 0.1070\n",
      "Epoch 13/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0965 - val_loss: 0.0900\n",
      "Epoch 14/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0886 - val_loss: 0.0871\n",
      "Epoch 15/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0822 - val_loss: 0.0904\n",
      "Epoch 16/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0768 - val_loss: 0.0786\n",
      "Epoch 17/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0719 - val_loss: 0.0834\n",
      "Epoch 18/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0676 - val_loss: 0.0711\n",
      "Epoch 19/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0639 - val_loss: 0.0652\n",
      "Epoch 20/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0600 - val_loss: 0.0618\n",
      "Epoch 21/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0571 - val_loss: 0.0612\n",
      "Epoch 22/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0541 - val_loss: 0.0579\n",
      "Epoch 23/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0516 - val_loss: 0.0540\n",
      "Epoch 24/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0491 - val_loss: 0.0522\n",
      "Epoch 25/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0469 - val_loss: 0.0513\n",
      "Epoch 26/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0448 - val_loss: 0.0508\n",
      "Epoch 27/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0431 - val_loss: 0.0492\n",
      "Epoch 28/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0412 - val_loss: 0.0490\n",
      "Epoch 29/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0396 - val_loss: 0.0464\n",
      "Epoch 30/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0384 - val_loss: 0.0437\n",
      "Epoch 31/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0367 - val_loss: 0.0432\n",
      "Epoch 32/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0354 - val_loss: 0.0421\n",
      "Epoch 33/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0342 - val_loss: 0.0436\n",
      "Epoch 34/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0330 - val_loss: 0.0421\n",
      "Epoch 35/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0319 - val_loss: 0.0389\n",
      "Epoch 36/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0310 - val_loss: 0.0389\n",
      "Epoch 37/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0300 - val_loss: 0.0381\n",
      "Epoch 38/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0291 - val_loss: 0.0381\n",
      "Epoch 39/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0281 - val_loss: 0.0375\n",
      "Epoch 40/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0274 - val_loss: 0.0376\n",
      "Epoch 41/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0266 - val_loss: 0.0350\n",
      "Epoch 42/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0259 - val_loss: 0.0366\n",
      "Epoch 43/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0253 - val_loss: 0.0345\n",
      "Epoch 44/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0246 - val_loss: 0.0345\n",
      "Epoch 45/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0240 - val_loss: 0.0382\n",
      "Epoch 46/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0233 - val_loss: 0.0351\n",
      "Epoch 47/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0228 - val_loss: 0.0362\n",
      "Epoch 48/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0222 - val_loss: 0.0331\n",
      "Epoch 49/50\n",
      "229272/229272 [==============================] - 17s 76us/step - loss: 0.0216 - val_loss: 0.0333\n",
      "Epoch 50/50\n",
      "229272/229272 [==============================] - 17s 75us/step - loss: 0.0213 - val_loss: 0.0325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa32019ed68>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution** I ran this model on GPU 1070. In cpu, It will take more than 5 min for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alireza/tf14/lib/python3.5/site-packages/keras/engine/topology.py:2368: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model.save('typo-corrector.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "For evaluation, we need to run a separate graph. Because in this step, we don't have ground truth word and decoder should guess next character, based on previous guessed character (dashed line in the picture above). It will continue until we reach the maximum time step or decoder guess a `\\n` character. \n",
    "\n",
    "It is good to point it out which decoder in training phase will use ground truth character at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, char2idx['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2char[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(word):\n",
    "    oh_word = np.zeros((max_encoder_seq_length, num_encoder_tokens),dtype=np.float32)\n",
    "    for i, c in enumerate(word):\n",
    "        oh_word[i,char2idx[c]] = 1.\n",
    "    return oh_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Typo word: capabilitoes\n",
      "True word: \tcapabilities\n",
      "\n",
      "Decoded word: capabilities\n",
      "\n",
      "-\n",
      "Typo word: promptong\n",
      "True word: \tprompting\n",
      "\n",
      "Decoded word: prompting\n",
      "\n",
      "-\n",
      "Typo word: srated\n",
      "True word: \tstated\n",
      "\n",
      "Decoded word: stated\n",
      "\n",
      "-\n",
      "Typo word: saited\n",
      "True word: \twaited\n",
      "\n",
      "Decoded word: waited\n",
      "\n",
      "-\n",
      "Typo word: qteps\n",
      "True word: \tsteps\n",
      "\n",
      "Decoded word: steps\n",
      "\n",
      "-\n",
      "Typo word: glags\n",
      "True word: \tflags\n",
      "\n",
      "Decoded word: flags\n",
      "\n",
      "-\n",
      "Typo word: xpeculated\n",
      "True word: \tspeculated\n",
      "\n",
      "Decoded word: speculated\n",
      "\n",
      "-\n",
      "Typo word: strstegist\n",
      "True word: \tstrategist\n",
      "\n",
      "Decoded word: streatents\n",
      "\n",
      "-\n",
      "Typo word: legislayive\n",
      "True word: \tlegislative\n",
      "\n",
      "Decoded word: legislative\n",
      "\n",
      "-\n",
      "Typo word: titlss\n",
      "True word: \ttitles\n",
      "\n",
      "Decoded word: titles\n",
      "\n",
      "-\n",
      "Typo word: hamfway\n",
      "True word: \thalfway\n",
      "\n",
      "Decoded word: hamfway\n",
      "\n",
      "-\n",
      "Typo word: albaniana\n",
      "True word: \talbanians\n",
      "\n",
      "Decoded word: albanians\n",
      "\n",
      "-\n",
      "Typo word: wiss\n",
      "True word: \twise\n",
      "\n",
      "Decoded word: wise\n",
      "\n",
      "-\n",
      "Typo word: sjzed\n",
      "True word: \tsized\n",
      "\n",
      "Decoded word: sized\n",
      "\n",
      "-\n",
      "Typo word: claqses\n",
      "True word: \tclasses\n",
      "\n",
      "Decoded word: classes\n",
      "\n",
      "-\n",
      "Typo word: remonstration\n",
      "True word: \tdemonstration\n",
      "\n",
      "Decoded word: demonstration\n",
      "\n",
      "-\n",
      "Typo word: predixting\n",
      "True word: \tpredicting\n",
      "\n",
      "Decoded word: predicting\n",
      "\n",
      "-\n",
      "Typo word: plx\n",
      "True word: \tplc\n",
      "\n",
      "Decoded word: pck\n",
      "\n",
      "-\n",
      "Typo word: visitprs\n",
      "True word: \tvisitors\n",
      "\n",
      "Decoded word: visitors\n",
      "\n",
      "-\n",
      "Typo word: monasterg\n",
      "True word: \tmonastery\n",
      "\n",
      "Decoded word: monastery\n",
      "\n",
      "-\n",
      "Typo word: citigrohp\n",
      "True word: \tcitigroup\n",
      "\n",
      "Decoded word: citigroup\n",
      "\n",
      "-\n",
      "Typo word: gyide\n",
      "True word: \tguide\n",
      "\n",
      "Decoded word: guide\n",
      "\n",
      "-\n",
      "Typo word: muxture\n",
      "True word: \tmixture\n",
      "\n",
      "Decoded word: mixture\n",
      "\n",
      "-\n",
      "Typo word: dusl\n",
      "True word: \tdual\n",
      "\n",
      "Decoded word: dual\n",
      "\n",
      "-\n",
      "Typo word: mrets\n",
      "True word: \tmeets\n",
      "\n",
      "Decoded word: metst\n",
      "\n",
      "-\n",
      "Typo word: disarmamemt\n",
      "True word: \tdisarmament\n",
      "\n",
      "Decoded word: disarmament\n",
      "\n",
      "-\n",
      "Typo word: cimedian\n",
      "True word: \tcomedian\n",
      "\n",
      "Decoded word: comedian\n",
      "\n",
      "-\n",
      "Typo word: riqe\n",
      "True word: \trise\n",
      "\n",
      "Decoded word: rise\n",
      "\n",
      "-\n",
      "Typo word: colombp\n",
      "True word: \tcolombo\n",
      "\n",
      "Decoded word: colombo\n",
      "\n",
      "-\n",
      "Typo word: effecy\n",
      "True word: \teffect\n",
      "\n",
      "Decoded word: effect\n",
      "\n",
      "-\n",
      "Typo word: loho\n",
      "True word: \tlogo\n",
      "\n",
      "Decoded word: logo\n",
      "\n",
      "-\n",
      "Typo word: rdgister\n",
      "True word: \tregister\n",
      "\n",
      "Decoded word: register\n",
      "\n",
      "-\n",
      "Typo word: poiwed\n",
      "True word: \tpoised\n",
      "\n",
      "Decoded word: poised\n",
      "\n",
      "-\n",
      "Typo word: negptiators\n",
      "True word: \tnegotiators\n",
      "\n",
      "Decoded word: negotiators\n",
      "\n",
      "-\n",
      "Typo word: vlinics\n",
      "True word: \tclinics\n",
      "\n",
      "Decoded word: clinics\n",
      "\n",
      "-\n",
      "Typo word: inaugueated\n",
      "True word: \tinaugurated\n",
      "\n",
      "Decoded word: inaugurated\n",
      "\n",
      "-\n",
      "Typo word: freeky\n",
      "True word: \tfreely\n",
      "\n",
      "Decoded word: freeky\n",
      "\n",
      "-\n",
      "Typo word: fylly\n",
      "True word: \tfully\n",
      "\n",
      "Decoded word: tylly\n",
      "\n",
      "-\n",
      "Typo word: wavihg\n",
      "True word: \twaving\n",
      "\n",
      "Decoded word: waving\n",
      "\n",
      "-\n",
      "Typo word: ceown\n",
      "True word: \tcrown\n",
      "\n",
      "Decoded word: crown\n",
      "\n",
      "-\n",
      "Typo word: carhival\n",
      "True word: \tcarnival\n",
      "\n",
      "Decoded word: carnival\n",
      "\n",
      "-\n",
      "Typo word: youreelf\n",
      "True word: \tyourself\n",
      "\n",
      "Decoded word: yourself\n",
      "\n",
      "-\n",
      "Typo word: entertaoning\n",
      "True word: \tentertaining\n",
      "\n",
      "Decoded word: entertaining\n",
      "\n",
      "-\n",
      "Typo word: ldon\n",
      "True word: \tleon\n",
      "\n",
      "Decoded word: leon\n",
      "\n",
      "-\n",
      "Typo word: secojd\n",
      "True word: \tsecond\n",
      "\n",
      "Decoded word: second\n",
      "\n",
      "-\n",
      "Typo word: ammujition\n",
      "True word: \tammunition\n",
      "\n",
      "Decoded word: ammunition\n",
      "\n",
      "-\n",
      "Typo word: higus\n",
      "True word: \thighs\n",
      "\n",
      "Decoded word: higus\n",
      "\n",
      "-\n",
      "Typo word: enckuraging\n",
      "True word: \tencouraging\n",
      "\n",
      "Decoded word: encouraging\n",
      "\n",
      "-\n",
      "Typo word: flowd\n",
      "True word: \tflows\n",
      "\n",
      "Decoded word: flows\n",
      "\n",
      "-\n",
      "Typo word: unxermine\n",
      "True word: \tundermine\n",
      "\n",
      "Decoded word: undermine\n",
      "\n",
      "-\n",
      "Typo word: vkices\n",
      "True word: \tvoices\n",
      "\n",
      "Decoded word: voices\n",
      "\n",
      "-\n",
      "Typo word: insigyt\n",
      "True word: \tinsight\n",
      "\n",
      "Decoded word: insight\n",
      "\n",
      "-\n",
      "Typo word: fiscover\n",
      "True word: \tdiscover\n",
      "\n",
      "Decoded word: discover\n",
      "\n",
      "-\n",
      "Typo word: strojes\n",
      "True word: \tstrokes\n",
      "\n",
      "Decoded word: strokes\n",
      "\n",
      "-\n",
      "Typo word: unfprtunate\n",
      "True word: \tunfortunate\n",
      "\n",
      "Decoded word: unfortunate\n",
      "\n",
      "-\n",
      "Typo word: zuburban\n",
      "True word: \tsuburban\n",
      "\n",
      "Decoded word: suburban\n",
      "\n",
      "-\n",
      "Typo word: litchen\n",
      "True word: \tkitchen\n",
      "\n",
      "Decoded word: litchen\n",
      "\n",
      "-\n",
      "Typo word: orikles\n",
      "True word: \torioles\n",
      "\n",
      "Decoded word: orioles\n",
      "\n",
      "-\n",
      "Typo word: poud\n",
      "True word: \tloud\n",
      "\n",
      "Decoded word: poud\n",
      "\n",
      "-\n",
      "Typo word: matn\n",
      "True word: \tmath\n",
      "\n",
      "Decoded word: math\n",
      "\n",
      "-\n",
      "Typo word: flamss\n",
      "True word: \tflames\n",
      "\n",
      "Decoded word: flames\n",
      "\n",
      "-\n",
      "Typo word: emrolled\n",
      "True word: \tenrolled\n",
      "\n",
      "Decoded word: enrolled\n",
      "\n",
      "-\n",
      "Typo word: sli\n",
      "True word: \tski\n",
      "\n",
      "Decoded word: ski\n",
      "\n",
      "-\n",
      "Typo word: bsnkrupt\n",
      "True word: \tbankrupt\n",
      "\n",
      "Decoded word: benkprit\n",
      "\n",
      "-\n",
      "Typo word: everythimg\n",
      "True word: \teverything\n",
      "\n",
      "Decoded word: everything\n",
      "\n",
      "-\n",
      "Typo word: fibrr\n",
      "True word: \tfiber\n",
      "\n",
      "Decoded word: fiber\n",
      "\n",
      "-\n",
      "Typo word: beara\n",
      "True word: \tbears\n",
      "\n",
      "Decoded word: bears\n",
      "\n",
      "-\n",
      "Typo word: acclrd\n",
      "True word: \taccord\n",
      "\n",
      "Decoded word: accord\n",
      "\n",
      "-\n",
      "Typo word: dsliver\n",
      "True word: \tdeliver\n",
      "\n",
      "Decoded word: deliver\n",
      "\n",
      "-\n",
      "Typo word: verxions\n",
      "True word: \tversions\n",
      "\n",
      "Decoded word: versions\n",
      "\n",
      "-\n",
      "Typo word: incrrased\n",
      "True word: \tincreased\n",
      "\n",
      "Decoded word: increased\n",
      "\n",
      "-\n",
      "Typo word: imtervene\n",
      "True word: \tintervene\n",
      "\n",
      "Decoded word: intervene\n",
      "\n",
      "-\n",
      "Typo word: custldy\n",
      "True word: \tcustody\n",
      "\n",
      "Decoded word: custody\n",
      "\n",
      "-\n",
      "Typo word: wonderfuk\n",
      "True word: \twonderful\n",
      "\n",
      "Decoded word: wonderful\n",
      "\n",
      "-\n",
      "Typo word: partjers\n",
      "True word: \tpartners\n",
      "\n",
      "Decoded word: partners\n",
      "\n",
      "-\n",
      "Typo word: decensive\n",
      "True word: \tdefensive\n",
      "\n",
      "Decoded word: defensive\n",
      "\n",
      "-\n",
      "Typo word: dxtradition\n",
      "True word: \textradition\n",
      "\n",
      "Decoded word: extradition\n",
      "\n",
      "-\n",
      "Typo word: heaxs\n",
      "True word: \theads\n",
      "\n",
      "Decoded word: heads\n",
      "\n",
      "-\n",
      "Typo word: inxtitutional\n",
      "True word: \tinstitutional\n",
      "\n",
      "Decoded word: institutional\n",
      "\n",
      "-\n",
      "Typo word: losimg\n",
      "True word: \tlosing\n",
      "\n",
      "Decoded word: losing\n",
      "\n",
      "-\n",
      "Typo word: tontue\n",
      "True word: \ttongue\n",
      "\n",
      "Decoded word: tongue\n",
      "\n",
      "-\n",
      "Typo word: debels\n",
      "True word: \trebels\n",
      "\n",
      "Decoded word: rebels\n",
      "\n",
      "-\n",
      "Typo word: expertq\n",
      "True word: \texperts\n",
      "\n",
      "Decoded word: experts\n",
      "\n",
      "-\n",
      "Typo word: ouystanding\n",
      "True word: \toutstanding\n",
      "\n",
      "Decoded word: outstanding\n",
      "\n",
      "-\n",
      "Typo word: mipitia\n",
      "True word: \tmilitia\n",
      "\n",
      "Decoded word: militia\n",
      "\n",
      "-\n",
      "Typo word: stealkng\n",
      "True word: \tstealing\n",
      "\n",
      "Decoded word: stealing\n",
      "\n",
      "-\n",
      "Typo word: enyrepreneur\n",
      "True word: \tentrepreneur\n",
      "\n",
      "Decoded word: entrepreneur\n",
      "\n",
      "-\n",
      "Typo word: kitcheh\n",
      "True word: \tkitchen\n",
      "\n",
      "Decoded word: kitchen\n",
      "\n",
      "-\n",
      "Typo word: prozimity\n",
      "True word: \tproximity\n",
      "\n",
      "Decoded word: proximity\n",
      "\n",
      "-\n",
      "Typo word: anonymlus\n",
      "True word: \tanonymous\n",
      "\n",
      "Decoded word: anonymous\n",
      "\n",
      "-\n",
      "Typo word: mailq\n",
      "True word: \tmails\n",
      "\n",
      "Decoded word: mails\n",
      "\n",
      "-\n",
      "Typo word: hopef\n",
      "True word: \thoped\n",
      "\n",
      "Decoded word: hoped\n",
      "\n",
      "-\n",
      "Typo word: executionx\n",
      "True word: \texecutions\n",
      "\n",
      "Decoded word: executions\n",
      "\n",
      "-\n",
      "Typo word: samzung\n",
      "True word: \tsamsung\n",
      "\n",
      "Decoded word: samsing\n",
      "\n",
      "-\n",
      "Typo word: weakdned\n",
      "True word: \tweakened\n",
      "\n",
      "Decoded word: weakened\n",
      "\n",
      "-\n",
      "Typo word: worket\n",
      "True word: \tworker\n",
      "\n",
      "Decoded word: worket\n",
      "\n",
      "-\n",
      "Typo word: willihg\n",
      "True word: \twilling\n",
      "\n",
      "Decoded word: willing\n",
      "\n",
      "-\n",
      "Typo word: typea\n",
      "True word: \ttypes\n",
      "\n",
      "Decoded word: types\n",
      "\n",
      "-\n",
      "Typo word: tolc\n",
      "True word: \ttold\n",
      "\n",
      "Decoded word: told\n",
      "\n",
      "-\n",
      "Typo word: fllors\n",
      "True word: \tfloors\n",
      "\n",
      "Decoded word: floors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Typo word:', inputs_data[seq_index])\n",
    "    print('True word:', outputs_data[seq_index])\n",
    "    print('Decoded word:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Typo word: thid\n",
      "Decoded word: this\n",
      "\n",
      "-\n",
      "Typo word: senrence\n",
      "Decoded word: sentence\n",
      "\n",
      "-\n",
      "Typo word: cintains\n",
      "Decoded word: contains\n",
      "\n",
      "-\n",
      "Typo word: manu\n",
      "Decoded word: many\n",
      "\n",
      "-\n",
      "Typo word: typi\n",
      "Decoded word: typi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_typo_sentence = \"thid senrence cintains manu typi\"\n",
    "my_words = my_typo_sentence.split()\n",
    "oh_words = []\n",
    "for w in my_words:\n",
    "    oh = get_one_hot(w)\n",
    "    oh = np.reshape(oh, (1, max_encoder_seq_length, num_encoder_tokens))\n",
    "    decoded_sentence = decode_sequence(oh)\n",
    "    print('-')\n",
    "    print('Typo word:', w)\n",
    "    print('Decoded word:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was interesting, isn't it? :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
